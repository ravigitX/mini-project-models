{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10821213,"sourceType":"datasetVersion","datasetId":6718702},{"sourceId":10891782,"sourceType":"datasetVersion","datasetId":6752008}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#i3d extraction\n\n!git clone https://github.com/piergiaj/pytorch-i3d.git\n%cd pytorch-i3d\n!pip install torch torchvision numpy pillow tqdm\n\n# Create models directory\n!mkdir -p models\n\n# Download flow_imagenet.pt into models/\n!wget -O /kaggle/working/flow_imagenet.pt https://www.dropbox.com/s/7w4z5q9fowcp9x5/flow_imagenet.pt?dl=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:46:00.870852Z","iopub.execute_input":"2025-04-06T12:46:00.871179Z","iopub.status.idle":"2025-04-06T12:46:05.769672Z","shell.execute_reply.started":"2025-04-06T12:46:00.871151Z","shell.execute_reply":"2025-04-06T12:46:05.768573Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'pytorch-i3d' already exists and is not an empty directory.\n/kaggle/working/pytorch-i3d\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n--2025-04-06 12:46:05--  https://www.dropbox.com/s/7w4z5q9fowcp9x5/flow_imagenet.pt?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘/kaggle/working/flow_imagenet.pt’\n\n/kaggle/working/flo     [ <=>                ]  70.18K  --.-KB/s    in 0.02s   \n\n2025-04-06 12:46:05 (3.47 MB/s) - ‘/kaggle/working/flow_imagenet.pt’ saved [71867]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/working/pytorch-i3d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:46:07.376062Z","iopub.execute_input":"2025-04-06T12:46:07.376365Z","iopub.status.idle":"2025-04-06T12:46:07.498404Z","shell.execute_reply.started":"2025-04-06T12:46:07.376341Z","shell.execute_reply":"2025-04-06T12:46:07.497566Z"}},"outputs":[{"name":"stdout","text":"charades_dataset_full.py  LICENSE.txt  pytorch_i3d.py  train_i3d.py\ncharades_dataset.py\t  models       README.md       videotransforms.py\nextract_features.py\t  __pycache__  state.db\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nfrom pytorch_i3d import InceptionI3d\n\nsys.path.append('/kaggle/working/pytorch-i3d')\n\n!wget -O /kaggle/working/flow_imagenet.pt https://github.com/piergiaj/pytorch-i3d/raw/master/models/flow_imagenet.pt\n!file /kaggle/working/flow_imagenet.pt\n!wget -O /kaggle/working/rgb_imagenet.pt https://github.com/piergiaj/pytorch-i3d/raw/master/models/rgb_imagenet.pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:46:09.995352Z","iopub.execute_input":"2025-04-06T12:46:09.995661Z","iopub.status.idle":"2025-04-06T12:46:15.181158Z","shell.execute_reply.started":"2025-04-06T12:46:09.995634Z","shell.execute_reply":"2025-04-06T12:46:15.180091Z"}},"outputs":[{"name":"stdout","text":"--2025-04-06 12:46:10--  https://github.com/piergiaj/pytorch-i3d/raw/master/models/flow_imagenet.pt\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/piergiaj/pytorch-i3d/master/models/flow_imagenet.pt [following]\n--2025-04-06 12:46:10--  https://raw.githubusercontent.com/piergiaj/pytorch-i3d/master/models/flow_imagenet.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 50795330 (48M) [application/octet-stream]\nSaving to: ‘/kaggle/working/flow_imagenet.pt’\n\n/kaggle/working/flo 100%[===================>]  48.44M   293MB/s    in 0.2s    \n\n2025-04-06 12:46:12 (293 MB/s) - ‘/kaggle/working/flow_imagenet.pt’ saved [50795330/50795330]\n\n/kaggle/working/flow_imagenet.pt: data\n--2025-04-06 12:46:12--  https://github.com/piergiaj/pytorch-i3d/raw/master/models/rgb_imagenet.pt\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/piergiaj/pytorch-i3d/master/models/rgb_imagenet.pt [following]\n--2025-04-06 12:46:13--  https://raw.githubusercontent.com/piergiaj/pytorch-i3d/master/models/rgb_imagenet.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 50883138 (49M) [application/octet-stream]\nSaving to: ‘/kaggle/working/rgb_imagenet.pt’\n\n/kaggle/working/rgb 100%[===================>]  48.53M   247MB/s    in 0.2s    \n\n2025-04-06 12:46:15 (247 MB/s) - ‘/kaggle/working/rgb_imagenet.pt’ saved [50883138/50883138]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom pytorch_i3d import InceptionI3d\nimport torchvision.transforms as transforms\n\n# --- Config ---\ndataset_path = '/kaggle/input/shanghaitech-anomaly-detection/dataset/mp'\noutput_dir = '/kaggle/working/i3d_train_rgb_features_output'\nmodel_weights = '/kaggle/working/rgb_imagenet.pt'  # Update with your RGB model weights\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Device ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nprint(f\"GPU Count: {torch.cuda.device_count()}\")\n\n# --- Load RGB I3D Model ---\ni3d = InceptionI3d(400, in_channels=3)  # Changed to 3 channels for RGB\ni3d.load_state_dict(torch.load(model_weights, weights_only=True))  # if you're loading only weights\ni3d.replace_logits(1024)\ni3d.to(device)\ni3d.eval()\n\n# --- Modified Feature Extractor for Single Frames ---\nclass I3DFrameFeatureExtractor(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def forward(self, x):\n        # Since we're processing single frames, we need to add temporal dimension\n        x = x.unsqueeze(2)  # (N, C, 1, H, W)\n        \n        # Forward pass through the model\n        for endpoint in self.model.end_points:\n            x = self.model.end_points[endpoint](x)\n            if endpoint == 'avg_pool':\n                break\n                \n        x = torch.nn.functional.adaptive_avg_pool3d(x, 1)  # (N, C, 1, 1, 1)\n        return x.view(x.size(0), -1)  # (N, C)\n\nextractor = I3DFrameFeatureExtractor(i3d).to(device)\n\n# --- Preprocessing for RGB frames ---\ndef preprocess_rgb_frame(frame_path):\n    img = Image.open(frame_path).resize((224, 224))\n    arr = np.array(img)\n    if len(arr.shape) == 2:  # Handle grayscale images\n        arr = np.stack([arr]*3, axis=-1)\n    arr = (arr / 127.5 - 1.0).astype(np.float32)  # Normalize to [-1, 1]\n    arr = torch.tensor(arr).permute(2, 0, 1)  # (C, H, W)\n    return arr\n\n# --- Extract Features from Single Frame ---\ndef extract_frame_feature(frame_path):\n    frame = preprocess_rgb_frame(frame_path).unsqueeze(0).to(device)  # (1, 3, H, W)\n    with torch.no_grad():\n        features = extractor(frame)\n    return features.cpu().numpy().squeeze()\n\n# --- Process Dataset ---\nvideo_folders = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n\nfor video_folder in tqdm(video_folders, desc=\"Extracting features\"):\n    frames_dir = os.path.join(dataset_path, video_folder, 'frames')  # Changed to frames directory\n    if not os.path.exists(frames_dir):\n        continue\n    \n    rgb_frames = sorted([\n        os.path.join(frames_dir, f) for f in os.listdir(frames_dir)\n        if f.endswith(('.jpg', '.png', '.jpeg'))\n    ])\n    \n    features = []\n    for frame_path in rgb_frames:\n        feat = extract_frame_feature(frame_path)\n        features.append(feat)\n    \n    features = np.array(features)\n    np.save(os.path.join(output_dir, f\"{video_folder}_rgb.npy\"), features)\n\nprint(\"✅ Single-frame RGB feature extraction complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FLOW FEATURE EXTRACTION**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom pytorch_i3d import InceptionI3d\nimport torchvision.transforms as transforms\n\n# --- Config ---\ndataset_path = '/kaggle/input/shanghaitech-anomaly-detection/dataset/mp'\noutput_dir = '/kaggle/working/i3d_test_flow_features_output'\nmodel_weights = '/kaggle/working/flow_imagenet.pt'\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Device ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Load Flow I3D Model ---\ni3d = InceptionI3d(400, in_channels=2)\ni3d.load_state_dict(torch.load(model_weights, weights_only=True))\ni3d.replace_logits(1024)\ni3d = torch.nn.DataParallel(i3d)\ni3d.to(device)\n\ni3d.eval()\n\nclass I3DFrameFeatureExtractor(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model.module  # 🔥 Access the actual model inside DataParallel\n\n    def forward(self, x):\n        x = x.unsqueeze(2)  # (N, C, 1, H, W)\n        for endpoint in self.model.end_points:\n            x = self.model.end_points[endpoint](x)\n            if endpoint == 'avg_pool':\n                break\n        x = torch.nn.functional.adaptive_avg_pool3d(x, 1)\n        return x.view(x.size(0), -1)\n\n\nextractor = I3DFrameFeatureExtractor(i3d).to(device)\n\n# --- Preprocessing (Flow uses 2 channels) ---\ndef preprocess_flow_frame(flow_path):\n    img = Image.open(flow_path).resize((224, 224))\n    arr = np.array(img)[:, :, :2]  # take only x/y flow channels\n    arr = (arr / 127.5 - 1.0).astype(np.float32)\n    arr = torch.tensor(arr).permute(2, 0, 1)  # (C, H, W)\n    return arr\n\n# --- Extract Features from Single Frame ---\ndef extract_frame_feature(frame_path):\n    frame = preprocess_flow_frame(frame_path).unsqueeze(0).to(device)  # (1, 2, H, W)\n    with torch.no_grad():\n        features = extractor(frame)\n    return features.cpu().numpy().squeeze()\n\n# --- Process Dataset ---\nvideo_folders = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n\nfor video_folder in tqdm(video_folders, desc=\"Extracting features\"):\n    flow_dir = os.path.join(dataset_path, video_folder, 'optical_flow')\n    if not os.path.exists(flow_dir):\n        continue\n    \n    flow_frames = sorted([\n        os.path.join(flow_dir, f) for f in os.listdir(flow_dir)\n        if f.endswith(('.jpg', '.png'))\n    ])\n    \n    features = []\n    for frame_path in flow_frames:\n        feat = extract_frame_feature(frame_path)\n        features.append(feat)\n    \n    features = np.array(features)\n    np.save(os.path.join(output_dir, f\"{video_folder}_flow.npy\"), features)\n\nprint(\"✅ Single-frame optical flow feature extraction complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:49:26.896340Z","iopub.execute_input":"2025-04-06T12:49:26.896725Z","iopub.status.idle":"2025-04-06T15:04:28.589209Z","shell.execute_reply.started":"2025-04-06T12:49:26.896696Z","shell.execute_reply":"2025-04-06T15:04:28.587859Z"}},"outputs":[{"name":"stderr","text":"Extracting features: 100%|██████████| 330/330 [2:15:01<00:00, 24.55s/it]  ","output_type":"stream"},{"name":"stdout","text":"✅ Single-frame optical flow feature extraction complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm import tqdm\n\n# --- Paths ---\nrgb_features_dir = '/kaggle/working/i3d_train_rgb_features_output'\nflow_features_dir = '/kaggle/working/i3d_test_flow_features_output'\noutput_dir = '/kaggle/working/combined_rgb_flow_features'\nos.makedirs(output_dir, exist_ok=True)\n\n# --- List available RGB feature files ---\nrgb_files = sorted([f for f in os.listdir(rgb_features_dir) if f.endswith('.npy')])\n\nfor rgb_file in tqdm(rgb_files, desc=\"Concatenating RGB + Flow features\"):\n    video_name = rgb_file.replace('_rgb.npy', '')\n    \n    rgb_path = os.path.join(rgb_features_dir, rgb_file)\n    flow_path = os.path.join(flow_features_dir, f\"{video_name}_flow.npy\")\n\n    if not os.path.exists(flow_path):\n        print(f\"⚠️ Skipping {video_name} — Flow feature missing.\")\n        continue\n\n    rgb_features = np.load(rgb_path)  # shape: (num_frames, feature_dim)\n    flow_features = np.load(flow_path)  # shape: (num_frames, feature_dim)\n\n    # Ensure same number of frames\n    min_len = min(len(rgb_features), len(flow_features))\n    rgb_features = rgb_features[:min_len]\n    flow_features = flow_features[:min_len]\n\n    # --- Concatenate along feature axis ---\n    combined_features = np.concatenate([rgb_features, flow_features], axis=1)  # shape: (num_frames, combined_feature_dim)\n\n    # --- Save ---\n    output_path = os.path.join(output_dir, f\"{video_name}_rgb_flow.npy\")\n    np.save(output_path, combined_features)\n\nprint(\"✅ All features successfully concatenated and saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}