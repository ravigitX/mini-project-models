{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5968d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T01:31:06.719293Z",
     "iopub.status.busy": "2025-03-11T01:31:06.718965Z",
     "iopub.status.idle": "2025-03-11T01:31:06.725755Z",
     "shell.execute_reply": "2025-03-11T01:31:06.724936Z"
    },
    "papermill": {
     "duration": 0.011649,
     "end_time": "2025-03-11T01:31:06.727225",
     "exception": false,
     "start_time": "2025-03-11T01:31:06.715576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def prevent_timeout(minutes=55):\n",
    "    \"\"\"\n",
    "    Prevents notebook timeout by printing a message every specified number of minutes.\n",
    "    Most notebook environments have a 60-90 minute timeout, so the default is set to 55 minutes.\n",
    "    \n",
    "    Args:\n",
    "        minutes (int): Number of minutes between each activity signal\n",
    "    \"\"\"\n",
    "    seconds = minutes * 60\n",
    "    counter = 1\n",
    "    \n",
    "    print(f\"Timeout prevention started. Will refresh every {minutes} minutes.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(seconds)\n",
    "            clear_output(wait=True)\n",
    "            current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"Keeping session alive... Ping #{counter} at {current_time}\")\n",
    "            print(f\"Timeout prevention active. Will refresh every {minutes} minutes.\")\n",
    "            counter += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Timeout prevention stopped.\")\n",
    "\n",
    "# Run this in a Jupyter notebook cell to start the timeout prevention\n",
    "# You can stop it by interrupting the kernel (Kernel > Interrupt) or by pressing Ctrl+C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b005fac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T01:31:06.732224Z",
     "iopub.status.busy": "2025-03-11T01:31:06.731989Z",
     "iopub.status.idle": "2025-03-11T09:11:08.150896Z",
     "shell.execute_reply": "2025-03-11T09:11:08.149536Z"
    },
    "papermill": {
     "duration": 27601.423888,
     "end_time": "2025-03-11T09:11:08.153170",
     "exception": false,
     "start_time": "2025-03-11T01:31:06.729282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 17137/17137 [21:51<00:00, 13.07it/s, Loss=0.0465, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] - MIL Loss: 1.6959 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 17137/17137 [17:54<00:00, 15.95it/s, Loss=1.1461, Acc=0.4444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25] - MIL Loss: 0.4441 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 17137/17137 [17:51<00:00, 16.00it/s, Loss=0.2520, Acc=0.8889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25] - MIL Loss: 0.4320 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 17137/17137 [17:33<00:00, 16.27it/s, Loss=0.0329, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25] - MIL Loss: 0.4269 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 17137/17137 [17:43<00:00, 16.12it/s, Loss=0.4708, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25] - MIL Loss: 0.4258 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 17137/17137 [17:31<00:00, 16.30it/s, Loss=0.0210, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25] - MIL Loss: 0.4249 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 17137/17137 [17:25<00:00, 16.40it/s, Loss=0.0164, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25] - MIL Loss: 0.4244 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 17137/17137 [17:12<00:00, 16.60it/s, Loss=0.6895, Acc=0.6667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25] - MIL Loss: 0.4240 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 17137/17137 [17:02<00:00, 16.75it/s, Loss=0.0251, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25] - MIL Loss: 0.4236 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 17137/17137 [17:07<00:00, 16.68it/s, Loss=0.2417, Acc=0.8889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25] - MIL Loss: 0.4234 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 17137/17137 [17:28<00:00, 16.34it/s, Loss=0.2427, Acc=0.8889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25] - MIL Loss: 0.4234 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 17137/17137 [17:44<00:00, 16.09it/s, Loss=0.6902, Acc=0.6667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25] - MIL Loss: 0.4234 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 17137/17137 [17:39<00:00, 16.17it/s, Loss=0.0170, Acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25] - MIL Loss: 0.4233 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 17137/17137 [17:49<00:00, 16.02it/s, Loss=0.2425, Acc=0.8889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25] - MIL Loss: 0.4233 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 17137/17137 [17:57<00:00, 15.90it/s, Loss=0.2462, Acc=0.8889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25] - MIL Loss: 0.4233 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 17137/17137 [17:49<00:00, 16.03it/s, Loss=0.6895, Acc=0.6667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25] - MIL Loss: 0.4232 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 17137/17137 [17:59<00:00, 15.88it/s, Loss=0.4665, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25] - MIL Loss: 0.4231 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 17137/17137 [17:40<00:00, 16.15it/s, Loss=1.1312, Acc=0.4444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25] - MIL Loss: 0.4227 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 17137/17137 [17:44<00:00, 16.09it/s, Loss=0.4656, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25] - MIL Loss: 0.4223 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 17137/17137 [17:30<00:00, 16.32it/s, Loss=0.4643, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25] - MIL Loss: 0.4216 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 17137/17137 [17:32<00:00, 16.28it/s, Loss=0.6879, Acc=0.6667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25] - MIL Loss: 0.4207 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 17137/17137 [17:23<00:00, 16.42it/s, Loss=0.4610, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25] - MIL Loss: 0.4191 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 17137/17137 [16:30<00:00, 17.30it/s, Loss=0.6771, Acc=0.6667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25] - MIL Loss: 0.4133 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 17137/17137 [16:34<00:00, 17.23it/s, Loss=0.4548, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25] - MIL Loss: 0.4115 - Accuracy: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 17137/17137 [16:34<00:00, 17.24it/s, Loss=0.4548, Acc=0.7778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25] - MIL Loss: 0.4114 - Accuracy: 0.7994\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Allow loading of truncated images\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Dataset Class\n",
    "class VideoAnomalyDataset(Dataset):\n",
    "    def __init__(self, base_folder, transform=None, use_optical_flow=False, \n",
    "                 combine_modalities=False, label_file=None):\n",
    "        \"\"\"\n",
    "        Dataset for loading video frames for anomaly detection.\n",
    "        \n",
    "        Args:\n",
    "            base_folder (str): Path to dataset containing video folders\n",
    "            transform (callable, optional): Transform to apply to frames\n",
    "            use_optical_flow (bool): Whether to use optical flow instead of RGB frames\n",
    "            combine_modalities (bool): Whether to combine RGB and optical flow (returns both)\n",
    "            label_file (str, optional): Path to annotation file with frame-level labels\n",
    "        \"\"\"\n",
    "        self.base_folder = base_folder\n",
    "        self.transform = transform\n",
    "        self.use_optical_flow = use_optical_flow\n",
    "        self.combine_modalities = combine_modalities\n",
    "        \n",
    "        # Find all video folders\n",
    "        self.video_folders = [f for f in os.listdir(base_folder) \n",
    "                             if os.path.isdir(os.path.join(base_folder, f))]\n",
    "        self.video_folders.sort()\n",
    "        \n",
    "        # Build frame paths and video indices for all videos\n",
    "        self.frame_paths = []\n",
    "        self.flow_paths = []\n",
    "        self.video_indices = []\n",
    "        \n",
    "        for video_idx, video_folder in enumerate(self.video_folders):\n",
    "            video_path = os.path.join(base_folder, video_folder)\n",
    "            \n",
    "            # Get frames for this video\n",
    "            frames_folder = os.path.join(video_path, 'frames')\n",
    "            if os.path.exists(frames_folder):\n",
    "                frames = [f for f in os.listdir(frames_folder) if f.endswith('.jpg')]\n",
    "                frames.sort()\n",
    "                \n",
    "                for frame in frames:\n",
    "                    self.frame_paths.append(os.path.join(frames_folder, frame))\n",
    "                    self.video_indices.append(video_idx)\n",
    "                    \n",
    "                    # Also get the corresponding optical flow if it exists\n",
    "                    flow_folder = os.path.join(video_path, 'optical_flow')\n",
    "                    flow_name = frame.replace('frame_', 'flow_')\n",
    "                    flow_path = os.path.join(flow_folder, flow_name)\n",
    "                    \n",
    "                    if os.path.exists(flow_path):\n",
    "                        self.flow_paths.append(flow_path)\n",
    "                    else:\n",
    "                        # If flow doesn't exist, use None as placeholder\n",
    "                        self.flow_paths.append(None)\n",
    "        \n",
    "        # Load labels if available\n",
    "        self.labels = self._load_labels(label_file)\n",
    "        \n",
    "        # If no labels provided, create dummy labels for demonstration\n",
    "        if self.labels is None:\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            self.labels = np.random.choice([0, 1], size=len(self.frame_paths), \n",
    "                                           p=[0.8, 0.2]).tolist()\n",
    "    \n",
    "    def _load_labels(self, label_file):\n",
    "        \"\"\"Load labels from file if available\"\"\"\n",
    "        if label_file is None or not os.path.exists(label_file):\n",
    "            return None\n",
    "            \n",
    "        # Implement label loading logic here based on your annotation format\n",
    "        # Example: CSV with frame_path,label format\n",
    "        labels = {}\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 2:\n",
    "                    frame_path, label = parts\n",
    "                    labels[frame_path] = int(label)\n",
    "                    \n",
    "        # Convert to list matching frame_paths order\n",
    "        return [labels.get(path, 0) for path in self.frame_paths]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = self.frame_paths[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        video_idx = self.video_indices[idx]\n",
    "        \n",
    "        try:\n",
    "            # Handle different modality options\n",
    "            if self.use_optical_flow and self.flow_paths[idx]:\n",
    "                # Use optical flow only\n",
    "                image = Image.open(self.flow_paths[idx]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "                \n",
    "            elif self.combine_modalities and self.flow_paths[idx]:\n",
    "                # Combine RGB frame and optical flow\n",
    "                frame = Image.open(frame_path).convert(\"RGB\")\n",
    "                flow = Image.open(self.flow_paths[idx]).convert(\"RGB\")\n",
    "                \n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                    flow = self.transform(flow)\n",
    "                \n",
    "                # Stack channels (can be modified based on how you want to combine)\n",
    "                combined = torch.cat((frame, flow), dim=0)  # Channel-wise concatenation\n",
    "                return combined, label\n",
    "                \n",
    "            else:\n",
    "                # Use RGB frame only (default)\n",
    "                frame = Image.open(frame_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                return frame, label\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {frame_path}: {e}\")\n",
    "            # Return a placeholder image and label\n",
    "            placeholder = torch.zeros(3, 112, 112) if self.transform is None else self.transform(\n",
    "                Image.new(\"RGB\", (112, 112), color=(0, 0, 0))\n",
    "            )\n",
    "            return placeholder, label\n",
    "    \n",
    "    def get_video_indices(self):\n",
    "        \"\"\"Return list of video indices for each frame\"\"\"\n",
    "        return self.video_indices\n",
    "    \n",
    "    def get_video_names(self):\n",
    "        \"\"\"Return list of video folder names\"\"\"\n",
    "        return self.video_folders\n",
    "\n",
    "# Modular Vision Transformer Implementation\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=112, patch_size=16, in_channels=3, embed_dim=384):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_heads=4, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim=384, hidden_dim=3072, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, embed_dim * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=112, patch_size=16, in_channels=3, embed_dim=384, depth=4, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (B, n_patches + 1, embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]  # Return only the class token\n",
    "\n",
    "# Define Vision Transformer Model\n",
    "class ViTForAnomalyDetection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTForAnomalyDetection, self).__init__()\n",
    "        self.vit = VisionTransformer(img_size=112, patch_size=16, embed_dim=384, depth=4, num_heads=4)\n",
    "       \n",
    "        # Fine-tuned fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)  # Output a single score for MIL\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize model with GPU support\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTForAnomalyDetection().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPU...\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Initialize model weights properly for better training\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        torch.nn.init.ones_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# MIL Loss Implementation\n",
    "class MILoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=0.01):\n",
    "        super(MILoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg  # Regularization strength\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"\n",
    "        outputs: Model predictions (batch_size, 1)\n",
    "        labels: Ground truth labels (batch_size,), where 1 = anomaly, 0 = normal\n",
    "        \"\"\"\n",
    "        # Convert labels to -1 (normal) and 1 (anomaly)\n",
    "        labels = 2 * labels.float() - 1  # 0 -> -1, 1 -> 1\n",
    "\n",
    "        # Hinge loss term\n",
    "        hinge_loss = torch.mean(torch.clamp(1 - labels * outputs, min=0))\n",
    "\n",
    "        # L2 regularization term (using model parameters)\n",
    "        l2_reg = 0.0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "\n",
    "        # Total MIL loss\n",
    "        mil_loss = hinge_loss + self.lambda_reg * l2_reg\n",
    "        return mil_loss\n",
    "\n",
    "criterion = MILoss(lambda_reg=0.01)\n",
    "\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Define functions to calculate accuracy and other metrics\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate accuracy from binary predictions and targets\n",
    "    \"\"\"\n",
    "    # Handle case when predictions is not a 1D tensor\n",
    "    if predictions.dim() > 1 and predictions.size(1) == 1:\n",
    "        predictions = predictions.squeeze(1)\n",
    "   \n",
    "    correct = (predictions == targets).float().sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def get_binary_predictions(outputs):\n",
    "    \"\"\"\n",
    "    Convert model outputs (scores) to binary predictions\n",
    "    \"\"\"\n",
    "    return (outputs > 0).float()\n",
    "\n",
    "# Define the dataset and dataloader\n",
    "base_folder = \"/kaggle/input/shanghaitech-anomaly-detection/dataset/mp\"  # Replace with the path to your dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize frames to 112x112\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),  # Normalize to mean 0.5, std 0.5\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = VideoAnomalyDataset(base_folder, transform=transform)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# Training Loop with Accuracy Calculation\n",
    "num_epochs = 25  # Increased epochs for better training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "   \n",
    "    for inputs, batch_labels in progress_bar:\n",
    "        inputs, batch_labels = inputs.to(device), batch_labels.to(device)\n",
    "       \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "       \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        # Get binary predictions from scores\n",
    "        binary_preds = get_binary_predictions(outputs)\n",
    "       \n",
    "        # Handle dimensionality of predictions\n",
    "        if binary_preds.dim() > 1 and binary_preds.size(1) == 1:\n",
    "            binary_preds = binary_preds.squeeze(1)\n",
    "           \n",
    "        # Calculate batch accuracy\n",
    "        batch_accuracy = calculate_accuracy(binary_preds, batch_labels)\n",
    "       \n",
    "        # Store predictions and labels for epoch-level metrics\n",
    "        all_preds.extend(binary_preds.cpu().detach().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().detach().numpy())\n",
    "       \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"Acc\": f\"{batch_accuracy:.4f}\"})\n",
    "   \n",
    "    # Calculate epoch metrics\n",
    "    epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    epoch_loss = running_loss / len(dataloader)  # Average loss for the epoch\n",
    "   \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "   \n",
    "    # Print epoch summary with explicit MIL loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - MIL Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Optional: Add validation here for more comprehensive evaluation\n",
    "   \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"vit_anomaly_detection_model.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6752008,
     "sourceId": 10891782,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27641.691279,
   "end_time": "2025-03-11T09:11:45.643488",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T01:31:03.952209",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
