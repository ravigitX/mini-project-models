{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11333890,"sourceType":"datasetVersion","datasetId":7089775},{"sourceId":11391601,"sourceType":"datasetVersion","datasetId":7134094}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ravikagglex/tds-enhanced-vad?scriptVersionId=233892637\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport csv\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport glob\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.315617Z","iopub.execute_input":"2025-04-14T17:44:28.315916Z","iopub.status.idle":"2025-04-14T17:44:28.320623Z","shell.execute_reply.started":"2025-04-14T17:44:28.315893Z","shell.execute_reply":"2025-04-14T17:44:28.319758Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class I3DFeaturesDataset(Dataset):\n    \"\"\"\n    Dataset for loading pre-extracted I3D features (combined RGB and Flow)\n    \"\"\"\n    def __init__(self, root_dir, is_train=True, segment_length=32):\n        \"\"\"\n        Args:\n            root_dir: Root directory of the I3D features\n            is_train: Whether to use training or testing data\n            segment_length: Number of frames per segment\n        \"\"\"\n        self.root_dir = root_dir\n        self.is_train = is_train\n        self.segment_length = segment_length\n        self.samples = []  # Will contain (feature_path, label)\n        \n        # Set paths based on train/test\n        if is_train:\n            base_dir = os.path.join(root_dir, '/kaggle/input/i3d-feats-train/I3D_Features_combined')\n        else:\n            base_dir = os.path.join(root_dir, '/kaggle/input/i3d-feats-test/mp/I3D_Features_combined')\n            \n        # Get combined feature directory\n        feature_dir = os.path.join(base_dir, '/kaggle/input')\n        \n        # Get all combined feature files\n        feature_files = sorted(glob.glob(os.path.join(feature_dir, 'combined_*_i3d.npy')))\n        \n        # Process feature files and assign labels\n        # Note: In a real implementation, you would need to map each file to its correct label\n        # For this example, we'll use a simple heuristic: files with odd indices are anomalous\n        for i, feature_file in enumerate(feature_files):\n            # Extract video ID from filename\n            video_id = os.path.basename(feature_file).split('_')[1]\n            # Assign label (this is just a placeholder - replace with your actual labeling logic)\n            label = 1 if int(video_id) % 2 == 1 else 0  # Odd video IDs are anomalous\n            self.samples.append((feature_file, label))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        feature_path, label = self.samples[idx]\n        # Load the feature file\n        features = np.load(feature_path)\n        \n        # Ensure features have the right shape for the model\n        # Assuming features have shape (num_frames, feature_dim)\n        # If num_frames > segment_length, we'll take a random segment\n        num_frames = features.shape[0]\n        if num_frames > self.segment_length:\n            start_idx = np.random.randint(0, num_frames - self.segment_length)\n            features = features[start_idx:start_idx + self.segment_length]\n        elif num_frames < self.segment_length:\n            # If fewer frames than needed, pad with zeros\n            padding = np.zeros((self.segment_length - num_frames, features.shape[1]))\n            features = np.concatenate([features, padding], axis=0)\n        \n        # Convert to tensor\n        features_tensor = torch.tensor(features, dtype=torch.float32)\n        label_tensor = torch.tensor(float(label), dtype=torch.float32)\n        \n        return features_tensor, label_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.321816Z","iopub.execute_input":"2025-04-14T17:44:28.322127Z","iopub.status.idle":"2025-04-14T17:44:28.351071Z","shell.execute_reply.started":"2025-04-14T17:44:28.322096Z","shell.execute_reply":"2025-04-14T17:44:28.350229Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# MLP used in transformer blocks\nclass MLP(nn.Module):\n    def __init__(self, in_features, hidden_features, dropout=0.1):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n# RTFM Module for anomaly detection with MIL ranking loss\nclass RTFMModule(nn.Module):\n    def __init__(self, feature_dim, hidden_dim=128, k=3, margin=1.0, lambda_margin=1.0):\n        \"\"\"\n        feature_dim: Dimension of snippet features\n        hidden_dim: Hidden dimension for the snippet classifier\n        k: Number of top snippets to select based on magnitude\n        margin: Desired margin between abnormal and normal snippet magnitudes\n        lambda_margin: Weight for the margin loss\n        \"\"\"\n        super(RTFMModule, self).__init__()\n        self.k = k\n        self.margin = margin\n        self.lambda_margin = lambda_margin\n        self.classifier = nn.Sequential(\n            nn.Linear(feature_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.bce_loss = nn.BCEWithLogitsLoss()\n    \n    def forward(self, x, labels=None):\n        \"\"\"\n        x: Tensor of snippet features, shape (B, T, feature_dim)\n        labels: Video-level labels, shape (B,) with 1 for anomaly, 0 for normal\n        \"\"\"\n        B, T, D = x.shape\n        magnitudes = torch.norm(x, p=2, dim=-1)  # (B, T)\n        topk_vals, topk_idx = torch.topk(magnitudes, self.k, dim=1)  # (B, k)\n        \n        # Get features for top-k snippets\n        batch_indices = torch.arange(B, device=x.device).unsqueeze(1).expand(-1, self.k)\n        topk_features = x[batch_indices, topk_idx]  # (B, k, D)\n        \n        # Apply classifier to top-k features\n        flat_features = topk_features.view(B * self.k, D)\n        topk_logits = self.classifier(flat_features)  # (B*k, 1)\n        topk_logits = topk_logits.view(B, self.k)  # (B, k)\n        \n        # Mean of top-k logits as video score\n        video_scores = topk_logits.mean(dim=1)  # (B,)\n        \n        if labels is None:\n            return video_scores, topk_logits, topk_vals\n        \n        # Calculate classification loss\n        classification_loss = self.bce_loss(video_scores, labels)\n        \n        # Calculate margin loss for feature magnitudes\n        abnormal_mask = (labels == 1)\n        normal_mask = (labels == 0)\n        margin_loss = torch.tensor(0.0, device=x.device)\n        \n        if abnormal_mask.sum() > 0 and normal_mask.sum() > 0:\n            abnormal_mag = topk_vals[abnormal_mask].mean()\n            normal_mag = topk_vals[normal_mask].mean()\n            diff = abnormal_mag - normal_mag\n            margin_loss = torch.clamp(self.margin - diff, min=0.0)\n        \n        total_loss = classification_loss + self.lambda_margin * margin_loss\n        return video_scores, topk_logits, topk_vals, total_loss\n\n# Transformer Encoder Block\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n        super(TransformerEncoderBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = MLP(embed_dim, hidden_dim, dropout=dropout)\n    \n    def forward(self, x):\n        # x: (B, N, embed_dim)\n        x = x + self.dropout1(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n        x = x + self.mlp(self.norm2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.352275Z","iopub.execute_input":"2025-04-14T17:44:28.352475Z","iopub.status.idle":"2025-04-14T17:44:28.381607Z","shell.execute_reply.started":"2025-04-14T17:44:28.352458Z","shell.execute_reply":"2025-04-14T17:44:28.380543Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class TDSNet(nn.Module):\n    def __init__(self, feature_dim=1024, num_frames=32, num_heads=8, \n                 num_layers=4, dropout=0.1):\n        \"\"\"\n        feature_dim: Dimension of I3D features\n        num_frames: Number of frames per segment\n        num_heads: Number of attention heads in transformer\n        num_layers: Number of transformer layers\n        dropout: Dropout rate\n        \"\"\"\n        super(TDSNet, self).__init__()\n        self.feature_dim = feature_dim\n        \n        # Feature projection\n        self.feature_projection = nn.Linear(self.feature_dim, self.feature_dim)\n        \n        # Position encoding\n        self.pos_embedding = nn.Parameter(torch.zeros(1, num_frames, self.feature_dim))\n        \n        # Transformer encoder\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoderBlock(self.feature_dim, num_heads, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Final layer norm\n        self.norm = nn.LayerNorm(self.feature_dim)\n        \n        # RTFM module for anomaly detection\n        self.rtfm_module = RTFMModule(feature_dim=self.feature_dim, hidden_dim=256, \n                                     k=3, margin=1.0, lambda_margin=1.0)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        # Initialize position embedding\n        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n        \n        # Initialize linear layers\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x, labels=None):\n        \"\"\"\n        x: Combined features (B, T, feature_dim)\n        labels: Video-level labels (B,)\n        \"\"\"\n        # Project features\n        x = self.feature_projection(x)\n        \n        # Add positional embedding\n        x = x + self.pos_embedding\n        \n        # Pass through transformer blocks\n        for block in self.transformer_blocks:\n            x = block(x)\n        \n        # Apply final normalization\n        x = self.norm(x)\n        \n        # Pass to RTFM module for anomaly detection\n        if labels is not None:\n            video_scores, topk_logits, topk_magnitudes, loss = self.rtfm_module(x, labels)\n            return video_scores, topk_logits, topk_magnitudes, loss\n        else:\n            video_scores, topk_logits, topk_magnitudes = self.rtfm_module(x, labels)\n            return video_scores, topk_logits, topk_magnitudes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.465576Z","iopub.execute_input":"2025-04-14T17:44:28.465871Z","iopub.status.idle":"2025-04-14T17:44:28.473946Z","shell.execute_reply.started":"2025-04-14T17:44:28.465845Z","shell.execute_reply":"2025-04-14T17:44:28.472894Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    running_anom = 0.0\n    num_batches = 0\n    \n    # Cumulative counters for metrics\n    TP_total = 0.0\n    FP_total = 0.0\n    FN_total = 0.0\n    \n    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n    for features, labels in pbar:\n        features = features.to(device)  # (B, T, D)\n        labels = labels.to(device)  # (B,)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        video_scores, topk_logits, topk_magnitudes, loss = model(features, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        # Calculate metrics\n        preds = (torch.sigmoid(video_scores) > 0.5).float()\n        batch_acc = (preds == labels).float().mean().item()\n        running_acc += batch_acc\n        \n        # Cumulative counts\n        TP = ((preds == 1) & (labels == 1)).float().sum().item()\n        FP = ((preds == 1) & (labels == 0)).float().sum().item()\n        FN = ((preds == 0) & (labels == 1)).float().sum().item()\n        TP_total += TP\n        FP_total += FP\n        FN_total += FN\n        \n        overall_precision = TP_total / (TP_total + FP_total + 1e-7)\n        overall_iou = TP_total / (TP_total + FP_total + FN_total + 1e-7)\n        \n        avg_anom = torch.sigmoid(video_scores).mean().item()\n        running_anom += avg_anom\n        running_loss += loss.item()\n        num_batches += 1\n        \n        pbar.set_postfix({\n            \"loss\": f\"{running_loss/num_batches:.4f}\",\n            \"acc\": f\"{running_acc/num_batches:.4f}\",\n            \"prec\": f\"{overall_precision:.4f}\",\n            \"iou\": f\"{overall_iou:.4f}\",\n            \"anom\": f\"{running_anom/num_batches:.4f}\"\n        })\n    \n    epoch_loss = running_loss / num_batches\n    epoch_acc = running_acc / num_batches\n    epoch_prec = overall_precision\n    epoch_iou = overall_iou\n    epoch_anom = running_anom / num_batches\n    \n    return epoch_loss, epoch_acc, epoch_prec, epoch_iou, epoch_anom\n\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    running_anom = 0.0\n    num_batches = 0\n    TP_total = 0.0\n    FP_total = 0.0\n    FN_total = 0.0\n    all_preds = []\n    all_labels = []\n    all_scores = []\n    \n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n        for features, labels in pbar:\n            features = features.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            video_scores, topk_logits, topk_magnitudes, loss = model(features, labels)\n            \n            # Calculate metrics\n            preds = (torch.sigmoid(video_scores) > 0.5).float()\n            batch_acc = (preds == labels).float().mean().item()\n            running_acc += batch_acc\n            \n            # Cumulative counts\n            TP = ((preds == 1) & (labels == 1)).float().sum().item()\n            FP = ((preds == 1) & (labels == 0)).float().sum().item()\n            FN = ((preds == 0) & (labels == 1)).float().sum().item()\n            TP_total += TP\n            FP_total += FP\n            FN_total += FN\n            \n            overall_precision = TP_total / (TP_total + FP_total + 1e-7)\n            overall_iou = TP_total / (TP_total + FP_total + FN_total + 1e-7)\n            \n            avg_anom = torch.sigmoid(video_scores).mean().item()\n            running_anom += avg_anom\n            running_loss += loss.item()\n            num_batches += 1\n            \n            # Store predictions and labels for ROC-AUC calculation\n            all_preds.extend(preds.cpu().numpy().tolist())\n            all_labels.extend(labels.cpu().numpy().tolist())\n            all_scores.extend(torch.sigmoid(video_scores).cpu().numpy().tolist())\n            \n            pbar.set_postfix({\n                \"loss\": f\"{running_loss/num_batches:.4f}\",\n                \"acc\": f\"{running_acc/num_batches:.4f}\",\n                \"prec\": f\"{overall_precision:.4f}\",\n                \"iou\": f\"{overall_iou:.4f}\",\n                \"anom\": f\"{running_anom/num_batches:.4f}\"\n            })\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(all_labels, all_preds)\n        print(\"Confusion Matrix (Validation):\")\n        print(cm)\n        \n        # Calculate AUC if possible\n        try:\n            auc = roc_auc_score(all_labels, all_scores)\n            print(f\"ROC-AUC: {auc:.4f}\")\n        except:\n            print(\"Could not calculate ROC-AUC\")\n        \n        # Calculate precision, recall, F1\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n        \n    epoch_loss = running_loss / num_batches\n    epoch_acc = running_acc / num_batches\n    epoch_prec = overall_precision\n    epoch_iou = overall_iou\n    epoch_anom = running_anom / num_batches\n    \n    return epoch_loss, epoch_acc, epoch_prec, epoch_iou, epoch_anom\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.475234Z","iopub.execute_input":"2025-04-14T17:44:28.475515Z","iopub.status.idle":"2025-04-14T17:44:28.499859Z","shell.execute_reply.started":"2025-04-14T17:44:28.475494Z","shell.execute_reply":"2025-04-14T17:44:28.49924Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def main():\n    # Hyperparameters and paths\n    root_dir = \"/kaggle/input\"  # Change to your dataset root folder\n    segment_length = 32\n    batch_size = 16\n    num_epochs = 100\n    learning_rate = 1e-4\n    checkpoint_dir = \"/kaggle/working/\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    csv_filename = os.path.join(checkpoint_dir, \"metrics.csv\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Create datasets\n    train_dataset = I3DFeaturesDataset(\n        root_dir=root_dir, \n        is_train=True, \n        segment_length=segment_length\n    )\n    test_dataset = I3DFeaturesDataset(\n        root_dir=root_dir, \n        is_train=False, \n        segment_length=segment_length\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=4, \n        pin_memory=True\n    )\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=4, \n        pin_memory=True\n    )\n    \n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Testing samples: {len(test_dataset)}\")\n    \n    # Initialize model\n    feature_dim = 1024  # Dimension of I3D features\n    model = TDSNet(\n        feature_dim=feature_dim, \n        num_frames=segment_length, \n        num_heads=8, \n        num_layers=4, \n        dropout=0.1\n    ).to(device)\n    \n    # Initialize optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=5, \n        verbose=True\n    )\n    \n    # Resume from checkpoint if available\n    start_epoch = 1\n    best_val_loss = float('inf')\n    resume_checkpoint = None  # Set to checkpoint path if resuming\n    \n    if resume_checkpoint and os.path.isfile(resume_checkpoint):\n        checkpoint = torch.load(resume_checkpoint, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_val_loss = checkpoint['val_loss']\n        print(f\"Resumed from epoch {start_epoch} with validation loss {best_val_loss:.4f}\")\n    \n    # Prepare CSV file for metrics\n    fieldnames = ['epoch', 'train_loss', 'train_acc', 'train_prec', 'train_iou', 'train_anom',\n                 'val_loss', 'val_acc', 'val_prec', 'val_iou', 'val_anom']\n    with open(csv_filename, mode='w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader()\n    \n    # Early stopping parameters\n    patience = 20\n    epochs_no_improve = 0\n    \n    # Training loop\n    for epoch in range(start_epoch, num_epochs + 1):\n        print(f\"Epoch {epoch}/{num_epochs}\")\n        \n        # Train\n        train_loss, train_acc, train_prec, train_iou, train_anom = train_epoch(\n            model, \n            train_loader, \n            optimizer, \n            device\n        )\n        \n        # Validate\n        val_loss, val_acc, val_prec, val_iou, val_anom = validate_epoch(\n            model, \n            test_loader, \n            device\n        )\n        \n        # Print metrics\n        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} Prec: {train_prec:.4f} IoU: {train_iou:.4f} Anom: {train_anom:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} Prec: {val_prec:.4f} IoU: {val_iou:.4f} Anom: {val_anom:.4f}\")\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Save metrics to CSV\n        with open(csv_filename, mode='a', newline='') as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n            writer.writerow({\n                'epoch': epoch,\n                'train_loss': train_loss,\n                'train_acc': train_acc,\n                'train_prec': train_prec,\n                'train_iou': train_iou,\n                'train_anom': train_anom,\n                'val_loss': val_loss,\n                'val_acc': val_acc,\n                'val_prec': val_prec,\n                'val_iou': val_iou,\n                'val_anom': val_anom\n            })\n        \n        # Save checkpoint if validation loss improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n            checkpoint_path = os.path.join(checkpoint_dir, f\"best_model_epoch_{epoch}.pth\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss\n            }, checkpoint_path)\n            print(f\"Checkpoint saved at {checkpoint_path}\")\n        else:\n            epochs_no_improve += 1\n        \n        # Early stopping\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered. No improvement in validation loss for {patience} epochs.\")\n            break\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.501181Z","iopub.execute_input":"2025-04-14T17:44:28.501426Z","iopub.status.idle":"2025-04-14T17:44:28.55115Z","shell.execute_reply.started":"2025-04-14T17:44:28.501405Z","shell.execute_reply":"2025-04-14T17:44:28.549983Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-84d0f80b1f8d>\u001b[0m in \u001b[0;36m<cell line: 155>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-84d0f80b1f8d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     train_loader = DataLoader(\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"],"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\n\ndef plot_metrics(csv_filename):\n    \"\"\"\n    Plot training and validation metrics from the CSV file\n    \"\"\"\n    # Read metrics from CSV\n    epochs = []\n    train_loss = []\n    val_loss = []\n    train_acc = []\n    val_acc = []\n    \n    with open(csv_filename, 'r') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            epochs.append(int(row['epoch']))\n            train_loss.append(float(row['train_loss']))\n            val_loss.append(float(row['val_loss']))\n            train_acc.append(float(row['train_acc']))\n            val_acc.append(float(row['val_acc']))\n    \n    # Create figure with subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot loss\n    ax1.plot(epochs, train_loss, 'b-', label='Training Loss')\n    ax1.plot(epochs, val_loss, 'r-', label='Validation Loss')\n    ax1.set_title('Loss vs. Epochs')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Plot accuracy\n    ax2.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n    ax2.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n    ax2.set_title('Accuracy vs. Epochs')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n    ax2.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.show()\n\ndef evaluate_model(model, dataloader, device):\n    \"\"\"\n    Evaluate model and plot ROC and Precision-Recall curves\n    \"\"\"\n    model.eval()\n    all_labels = []\n    all_scores = []\n    \n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            features = features.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            video_scores, _, _, _ = model(features, labels)\n            \n            # Store predictions and labels\n            all_scores.extend(torch.sigmoid(video_scores).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Convert to numpy arrays\n    all_labels = np.array(all_labels)\n    all_scores = np.array(all_scores)\n    \n    # Calculate ROC curve and AUC\n    fpr, tpr, _ = roc_curve(all_labels, all_scores)\n    roc_auc = auc(fpr, tpr)\n    \n    # Calculate Precision-Recall curve and AUC\n    precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n    pr_auc = auc(recall, precision)\n    \n    # Plot ROC curve\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    \n    # Plot Precision-Recall curve\n    plt.subplot(1, 2, 2)\n    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend(loc=\"lower left\")\n    \n    plt.tight_layout()\n    plt.savefig('evaluation_curves.png')\n    plt.show()\n    \n    return roc_auc, pr_auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T17:44:28.551686Z","iopub.status.idle":"2025-04-14T17:44:28.551946Z","shell.execute_reply":"2025-04-14T17:44:28.551848Z"}},"outputs":[],"execution_count":null}]}