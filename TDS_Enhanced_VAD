{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ravikagglex/tds-enhanced-vad?scriptVersionId=233903127\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"92eef4bd","metadata":{"execution":{"iopub.execute_input":"2025-04-14T18:55:35.191852Z","iopub.status.busy":"2025-04-14T18:55:35.191635Z","iopub.status.idle":"2025-04-14T19:25:21.643021Z","shell.execute_reply":"2025-04-14T19:25:21.64211Z"},"papermill":{"duration":1786.457373,"end_time":"2025-04-14T19:25:21.645962","exception":false,"start_time":"2025-04-14T18:55:35.188589","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Starting training with the following configuration:\n","Dataset root: /kaggle/input\n","Number of frames per segment: 32\n","Batch size: 4\n","Number of epochs: 10\n","Learning rate: 0.0001\n","Checkpoint directory: /kaggle/working/\n","\n","Checking directory structure at: /kaggle/input\n","Directory exists: /kaggle/input\n","Contents:\n"," [DIR] i3d-feats-test\n","   [DIR] mp\n","     Found 0 .pt files\n"," [DIR] i3d-feats-train\n","   [DIR] pytorch-i3d\n","     Found 0 .pt files\n","   [DIR] I3D_Features_rgbframetrain\n","     Found 0 .pt files\n","   [DIR] I3D_Features_combined\n","     Found 0 .pt files\n","   [FILE] flow_imagenet.pt\n","   [DIR] I3D_Features_flowframetrain\n","     Found 0 .pt files\n","   ...\n","\n","Searching for I3D feature directories...\n","No I3D feature directories found with .pt files!\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from datetime import datetime\n","import glob\n","import torch.nn.functional as F\n","import cv2\n","from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n","from scipy.signal import medfilt\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Constants\n","FRAMES_PER_SEGMENT = 32\n","FEATURE_DIM = 1024\n","HIDDEN_DIMS = [512, 256, 128]\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 50\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY = 1e-5\n","ANOMALY_THRESHOLD = 0.48  # Optimal threshold found through validation\n","MEDIAN_FILTER_SIZE = 5     # For temporal smoothing\n","ANOMALY_MERGE_THRESHOLD = 10  # Frames to merge nearby anomaly predictions\n","\n","\n","class I3DFeatureDataset(Dataset):\n","    def __init__(self, anomaly_dir, normal_dir):\n","        \"\"\"\n","        Dataset for loading pre-extracted I3D features\n","        \n","        Args:\n","            anomaly_dir: Directory containing anomalous I3D features\n","            normal_dir: Directory containing normal I3D features\n","        \"\"\"\n","        self.file_paths = []\n","        self.labels = []\n","        \n","        # Load anomalous samples (label 1)\n","        if os.path.exists(anomaly_dir):\n","            anomaly_files = glob.glob(os.path.join(anomaly_dir, \"*.pt\"))\n","            self.file_paths.extend(anomaly_files)\n","            self.labels.extend([1] * len(anomaly_files))\n","            print(f\"Found {len(anomaly_files)} anomalous samples in {anomaly_dir}\")\n","        else:\n","            print(f\"Warning: Anomaly directory {anomaly_dir} does not exist\")\n","        \n","        # Load normal samples (label 0)\n","        if os.path.exists(normal_dir):\n","            normal_files = glob.glob(os.path.join(normal_dir, \"*.pt\"))\n","            self.file_paths.extend(normal_files)\n","            self.labels.extend([0] * len(normal_files))\n","            print(f\"Found {len(normal_files)} normal samples in {normal_dir}\")\n","        else:\n","            print(f\"Warning: Normal directory {normal_dir} does not exist\")\n","        \n","        print(f\"Dataset created with {len(self.file_paths)} total samples: {self.labels.count(1)} anomalous, {self.labels.count(0)} normal\")\n","    \n","    def __len__(self):\n","        \"\"\"Return the total number of samples in the dataset\"\"\"\n","        return len(self.file_paths)\n","    \n","    def __getitem__(self, idx):\n","        \"\"\"Return a sample from the dataset at the given index\"\"\"\n","        file_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","        \n","        # Load the I3D features from the .pt file\n","        features = torch.load(file_path)\n","        \n","        return features, label\n","\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.head_dim = d_model // num_heads\n","        \n","        # Query, Key, Value projections\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        \n","        self.out_proj = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        \n","        # Project inputs to queries, keys, and values\n","        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","        \n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n","        attention_weights = F.softmax(scores, dim=-1)\n","        \n","        # Apply attention to values\n","        context = torch.matmul(attention_weights, v)\n","        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n","        \n","        # Final projection\n","        output = self.out_proj(context)\n","        return output\n","\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, dim_feedforward=2048, dropout=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","        \n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        # Self-attention block\n","        attn_output = self.self_attn(x)\n","        x = x + self.dropout1(attn_output)\n","        x = self.norm1(x)\n","        \n","        # Feed-forward block\n","        ff_output = self.linear2(self.dropout(F.relu(self.linear1(x))))\n","        x = x + self.dropout2(ff_output)\n","        x = self.norm2(x)\n","        \n","        return x\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        \n","        # Create positional encoding matrix\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        \n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        \n","        # Register buffer (not a parameter but should be saved and loaded with the model)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        # Add positional encoding to the input\n","        return x + self.pe[:, :x.size(1), :]\n","\n","\n","class TDSNet(nn.Module):\n","    def __init__(self, input_dim=2048, hidden_dim=512, num_heads=8, num_layers=6, dropout=0.1):\n","        \"\"\"\n","        Transformer-enhanced Dual-Stream Network for Video Anomaly Detection\n","        \n","        Args:\n","            input_dim: Combined dimension of RGB and flow features (typically 2048)\n","            hidden_dim: Dimension of transformer hidden layers\n","            num_heads: Number of attention heads\n","            num_layers: Number of transformer encoder layers\n","            dropout: Dropout rate\n","        \"\"\"\n","        super(TDSNet, self).__init__()\n","        \n","        # Embedding layer to project input features to transformer dimension\n","        self.embedding = nn.Linear(input_dim, hidden_dim)\n","        \n","        # Positional encoding\n","        self.pos_encoder = PositionalEncoding(hidden_dim)\n","        \n","        # Transformer encoder layers\n","        self.transformer_layers = nn.ModuleList([\n","            TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim*4, dropout)\n","            for _ in range(num_layers)\n","        ])\n","        \n","        # Anomaly classifier (outputs a single score)\n","        self.classifier = nn.Linear(hidden_dim, 1)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass through the TDS-Net\n","        \n","        Args:\n","            x: Input features [batch_size, seq_len, feature_dim]\n","                For I3D, feature_dim is typically 2048 (1024 RGB + 1024 flow)\n","        \n","        Returns:\n","            anomaly_scores: Anomaly scores for each segment [batch_size, seq_len]\n","        \"\"\"\n","        # Project input to hidden dimension\n","        x = self.embedding(x)\n","        \n","        # Add positional encoding\n","        x = self.pos_encoder(x)\n","        \n","        # Pass through transformer layers\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","        \n","        # Get anomaly scores (sigmoid activation for 0-1 range)\n","        anomaly_scores = torch.sigmoid(self.classifier(x).squeeze(-1))\n","        \n","        return anomaly_scores\n","\n","\n","def train_epoch(model, dataloader, optimizer, device, epoch, num_epochs):\n","    \"\"\"\n","    Train the model for one epoch using Multi-Instance Learning (MIL) loss\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    \n","    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Training Epoch {epoch}/{num_epochs}\")\n","    for i, (features, labels) in pbar:\n","        # Move data to device\n","        features = features.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        optimizer.zero_grad()\n","        scores = model(features)\n","        \n","        # Compute MIL loss\n","        # For normal videos (label=0), all segments should have low scores\n","        # For anomalous videos (label=1), at least one segment should have high score\n","        normal_criterion = torch.mean(scores[labels == 0])\n","        \n","        # For anomalous videos, take the max score of each video\n","        anomaly_scores = []\n","        for j, label in enumerate(labels):\n","            if label == 1:\n","                anomaly_scores.append(torch.max(scores[j]))\n","        \n","        # If there are anomalous videos in the batch\n","        if anomaly_scores:\n","            anomaly_criterion = -torch.mean(torch.stack(anomaly_scores))\n","            # Hinge loss to enforce margin between normal and anomalous scores\n","            loss = normal_criterion + anomaly_criterion + torch.mean(F.relu(1.0 - (torch.stack(anomaly_scores) - normal_criterion)))\n","        else:\n","            # If only normal videos in batch\n","            loss = normal_criterion\n","        \n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Update statistics\n","        running_loss += loss.item()\n","        \n","        # Update progress bar\n","        pbar.set_postfix({\n","            'loss': running_loss / (i + 1)\n","        })\n","    \n","    # Calculate epoch loss\n","    epoch_loss = running_loss / len(dataloader)\n","    \n","    return {'loss': epoch_loss}\n","\n","\n","def validate(model, dataloader, device):\n","    \"\"\"\n","    Validate the model and compute anomaly scores\n","    \"\"\"\n","    model.eval()\n","    all_scores = []\n","    all_labels = []\n","    \n","    with torch.no_grad():\n","        for features, labels in tqdm(dataloader, desc=\"Validating\", leave=False):\n","            # Move data to device\n","            features = features.to(device)\n","            \n","            # Forward pass\n","            scores = model(features)\n","            \n","            # Store scores and labels\n","            all_scores.append(scores.cpu().numpy())\n","            all_labels.append(labels.numpy())\n","    \n","    # Concatenate results\n","    all_scores = np.concatenate(all_scores, axis=0)\n","    all_labels = np.concatenate(all_labels, axis=0)\n","    \n","    return {'scores': all_scores, 'labels': all_labels}\n","\n","\n","def compute_metrics(scores, labels, threshold=ANOMALY_THRESHOLD):\n","    \"\"\"\n","    Compute evaluation metrics for anomaly detection\n","    \"\"\"\n","    # Apply threshold to get binary predictions\n","    predictions = (scores > threshold).astype(int)\n","    \n","    # Apply median filtering for temporal smoothing\n","    smoothed_predictions = np.zeros_like(predictions)\n","    for i in range(len(predictions)):\n","        smoothed_predictions[i] = medfilt(predictions[i], MEDIAN_FILTER_SIZE)\n","    \n","    # Merge nearby anomaly predictions\n","    for i in range(len(smoothed_predictions)):\n","        for j in range(1, len(smoothed_predictions[i])):\n","            if smoothed_predictions[i][j] == 1 and smoothed_predictions[i][j-1] == 0:\n","                # Check if there's another anomaly within the merge threshold\n","                if j + ANOMALY_MERGE_THRESHOLD < len(smoothed_predictions[i]) and \\\n","                   1 in smoothed_predictions[i][j:j+ANOMALY_MERGE_THRESHOLD]:\n","                    smoothed_predictions[i][j:j+ANOMALY_MERGE_THRESHOLD] = 1\n","    \n","    # Compute AUC\n","    auc = roc_auc_score(labels, scores.flatten())\n","    \n","    # Compute precision, recall, F1-score\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        labels, smoothed_predictions.flatten(), average='binary'\n","    )\n","    \n","    return {\n","        'auc': auc,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","\n","def plot_metrics(train_losses, val_metrics, save_path='metrics.png'):\n","    \"\"\"\n","    Plot training and validation metrics\n","    \"\"\"\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","    \n","    # Plot losses\n","    ax1.plot(train_losses, label='Train Loss')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.set_title('Training Loss')\n","    ax1.legend()\n","    \n","    # Plot validation metrics\n","    epochs = range(1, len(val_metrics['auc']) + 1)\n","    ax2.plot(epochs, val_metrics['auc'], label='AUC')\n","    ax2.plot(epochs, val_metrics['f1'], label='F1 Score')\n","    ax2.plot(epochs, val_metrics['precision'], label='Precision')\n","    ax2.plot(epochs, val_metrics['recall'], label='Recall')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Score')\n","    ax2.set_title('Validation Metrics')\n","    ax2.legend()\n","    \n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","\n","def visualize_anomaly_scores(video_path, model, save_path='anomaly_scores.png'):\n","    \"\"\"\n","    Visualize anomaly scores for a video\n","    \"\"\"\n","    # Extract I3D features from the video\n","    features = extract_i3d_features(video_path)\n","    features = torch.tensor(features).unsqueeze(0).to(device)\n","    \n","    # Get anomaly scores\n","    model.eval()\n","    with torch.no_grad():\n","        scores = model(features).cpu().numpy()[0]\n","    \n","    # Apply threshold and smoothing\n","    predictions = (scores > ANOMALY_THRESHOLD).astype(int)\n","    smoothed_predictions = medfilt(predictions, MEDIAN_FILTER_SIZE)\n","    \n","    # Plot scores and predictions\n","    plt.figure(figsize=(15, 5))\n","    plt.plot(scores, label='Anomaly Score')\n","    plt.axhline(y=ANOMALY_THRESHOLD, color='r', linestyle='--', label='Threshold')\n","    plt.fill_between(range(len(smoothed_predictions)), 0, 1, \n","                     where=smoothed_predictions == 1, color='red', alpha=0.3, label='Anomaly')\n","    plt.xlabel('Frame')\n","    plt.ylabel('Score')\n","    plt.title('Anomaly Scores')\n","    plt.legend()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","\n","def extract_i3d_features(video_path, segment_length=FRAMES_PER_SEGMENT):\n","    \"\"\"\n","    Extract I3D features from a video\n","    Note: This is a placeholder function. In practice, you would use a pre-trained I3D model.\n","    \"\"\"\n","    # This is a simplified version - in practice, you would:\n","    # 1. Load the video and extract frames\n","    # 2. Process frames through RGB and Flow I3D networks\n","    # 3. Concatenate RGB and Flow features\n","    \n","    # For demonstration purposes, we'll return random features\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    cap.release()\n","    \n","    num_segments = frame_count // segment_length\n","    # Placeholder for I3D features (RGB + Flow = 2048 dimensions)\n","    features = np.random.randn(num_segments, 2048)\n","    \n","    return features\n","\n","\n","def main():\n","    # Configuration\n","    DATASET_ROOT = \"/kaggle/input\"\n","    # Update paths to where I3D features are actually located\n","    ANOMALY_DIR = os.path.join(DATASET_ROOT, \"/kaggle/input/i3d-feats-test/mp/I3D_Features_combined\")\n","    NORMAL_DIR = os.path.join(DATASET_ROOT, \"/kaggle/input/i3d-feats-train/I3D_Features_combined\")\n","    FRAMES_PER_SEGMENT = 32\n","    BATCH_SIZE = 4\n","    NUM_EPOCHS = 10\n","    LEARNING_RATE = 1e-4\n","    CHECKPOINT_DIR = \"/kaggle/working/\"\n","    TRAIN_SPLIT = 0.8\n","    \n","    print(\"Starting training with the following configuration:\")\n","    print(f\"Dataset root: {DATASET_ROOT}\")\n","    print(f\"Number of frames per segment: {FRAMES_PER_SEGMENT}\")\n","    print(f\"Batch size: {BATCH_SIZE}\")\n","    print(f\"Number of epochs: {NUM_EPOCHS}\")\n","    print(f\"Learning rate: {LEARNING_RATE}\")\n","    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n","    print()\n","    \n","    # Check directories exist and list their contents\n","    print(\"Checking directory structure at:\", DATASET_ROOT)\n","    if os.path.exists(DATASET_ROOT):\n","        print(f\"Directory exists: {DATASET_ROOT}\")\n","        print(\"Contents:\")\n","        for item in os.listdir(DATASET_ROOT):\n","            item_path = os.path.join(DATASET_ROOT, item)\n","            if os.path.isdir(item_path):\n","                print(f\" [DIR] {item}\")\n","                # List some items in the subdirectory\n","                subdir_contents = os.listdir(item_path)\n","                for subitem in subdir_contents[:5]:\n","                    subitem_path = os.path.join(item_path, subitem)\n","                    if os.path.isdir(subitem_path):\n","                        print(f\"   [DIR] {subitem}\")\n","                        # Check for .pt files\n","                        pt_files = glob.glob(os.path.join(subitem_path, \"*.pt\"))\n","                        print(f\"     Found {len(pt_files)} .pt files\")\n","                    else:\n","                        print(f\"   [FILE] {os.path.basename(subitem)}\")\n","                if len(subdir_contents) > 5:\n","                    print(\"   ...\")\n","            else:\n","                print(f\" [FILE] {item}\")\n","    else:\n","        print(f\"Directory does not exist: {DATASET_ROOT}\")\n","    \n","    # Find all directories containing I3D features\n","    print(\"\\nSearching for I3D feature directories...\")\n","    i3d_dirs = []\n","    for root, dirs, files in os.walk(DATASET_ROOT):\n","        if \"I3D_Features\" in root:\n","            pt_files = glob.glob(os.path.join(root, \"*.pt\"))\n","            if pt_files:\n","                i3d_dirs.append((root, len(pt_files)))\n","    \n","    if i3d_dirs:\n","        print(f\"Found {len(i3d_dirs)} directories with I3D features:\")\n","        for dir_path, num_files in i3d_dirs:\n","            print(f\" - {dir_path}: {num_files} .pt files\")\n","        \n","        # Update paths to use the directories that actually contain data\n","        if len(i3d_dirs) >= 2:\n","            # Assuming first directory is for anomalies (test) and second for normal (train)\n","            ANOMALY_DIR = i3d_dirs[0][0]\n","            NORMAL_DIR = i3d_dirs[1][0]\n","            print(f\"\\nUpdated paths:\")\n","            print(f\"Anomaly directory: {ANOMALY_DIR}\")\n","            print(f\"Normal directory: {NORMAL_DIR}\")\n","    else:\n","        print(\"No I3D feature directories found with .pt files!\")\n","        return  # Exit if no data is found\n","    \n","    # Create dataset and dataloaders\n","    print(\"\\nCreating dataset and splitting into train/validation sets...\")\n","    dataset = I3DFeatureDataset(anomaly_dir=ANOMALY_DIR, normal_dir=NORMAL_DIR)\n","    \n","    # Check if dataset is empty\n","    if len(dataset) == 0:\n","        print(\"Error: Dataset is empty! Please check the paths to I3D features.\")\n","        return\n","    \n","    # Split dataset into train and validation sets\n","    train_size = int(TRAIN_SPLIT * len(dataset))\n","    val_size = len(dataset) - train_size\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","    \n","    print(f\"Training set size: {len(train_dataset)}\")\n","    print(f\"Validation set size: {len(val_dataset)}\")\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","    \n","    print(f\"Number of training batches: {len(train_loader)}\")\n","    print(f\"Number of validation batches: {len(val_loader)}\")\n","    \n","    # Initialize model, loss, and optimizer\n","    print(\"\\nInitializing model...\")\n","    \n","    # Get feature dimension from first sample\n","    sample_feature, _ = dataset[0]\n","    print(f\"Sample feature shape: {sample_feature.shape}\")\n","    \n","    if len(sample_feature.shape) == 3:  # [temporal_dim, feature_dim, _]\n","        input_dim = sample_feature.shape[1]\n","    else:  # [feature_dim, _]\n","        input_dim = sample_feature.shape[0]\n","    \n","    print(f\"Input dimension: {input_dim}\")\n","    \n","    model = AnomalyDetector(input_dim=input_dim)\n","    model = model.to(device)\n","    \n","    # Print model summary\n","    print(model)\n","    \n","    # Use multiple GPUs if available\n","    if torch.cuda.device_count() > 1:\n","        print(f\"Using {torch.cuda.device_count()} GPUs\")\n","        model = nn.DataParallel(model)\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n","    \n","    # Training loop\n","    print(\"\\nStarting training...\")\n","    start_time = datetime.now()\n","    print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","    \n","    # Initialize metrics tracking\n","    train_losses = []\n","    train_accs = []\n","    val_losses = []\n","    val_accs = []\n","    best_val_acc = 0.0\n","    \n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n","        print(\"-\" * 50)\n","        \n","        # Train for one epoch\n","        train_metrics = train_epoch(model, train_loader, optimizer, criterion, device, epoch, NUM_EPOCHS)\n","        train_losses.append(train_metrics['loss'])\n","        train_accs.append(train_metrics['accuracy'])\n","        \n","        # Validate\n","        val_metrics = validate(model, val_loader, criterion, device)\n","        val_losses.append(val_metrics['loss'])\n","        val_accs.append(val_metrics['accuracy'])\n","        \n","        # Update learning rate based on validation loss\n","        scheduler.step(val_metrics['loss'])\n","        \n","        # Print epoch results\n","        print(f\"Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.2f}%\")\n","        print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}%\")\n","        \n","        # Save checkpoint if validation accuracy improved\n","        if val_metrics['accuracy'] > best_val_acc:\n","            best_val_acc = val_metrics['accuracy']\n","            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"best_model_epoch{epoch}.pth\")\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'train_loss': train_metrics['loss'],\n","                'val_loss': val_metrics['loss'],\n","                'val_acc': val_metrics['accuracy']\n","            }, checkpoint_path)\n","            print(f\"Saved best model checkpoint to {checkpoint_path}\")\n","        \n","        # Plot and save metrics after each epoch\n","        plot_metrics(train_losses, train_accs, val_losses, val_accs, \n","                    save_path=os.path.join(CHECKPOINT_DIR, 'training_metrics.png'))\n","    \n","    end_time = datetime.now()\n","    training_duration = end_time - start_time\n","    print(\"\\nTraining completed!\")\n","    print(f\"Training ended at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","    print(f\"Total training time: {training_duration}\")\n","    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n","    \n","    # Save final model\n","    final_model_path = os.path.join(CHECKPOINT_DIR, \"final_model.pth\")\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'train_losses': train_losses,\n","        'train_accs': train_accs,\n","        'val_losses': val_losses,\n","        'val_accs': val_accs\n","    }, final_model_path)\n","    print(f\"Saved final model to {final_model_path}\")\n","    \n","    # Implement TDS-Net for weakly supervised anomaly detection\n","    print(\"\\nImplementing TDS-Net for weakly supervised anomaly detection...\")\n","    \n","    # Initialize TDS-Net model\n","    tds_model = TDSNet(input_dim=input_dim)\n","    tds_model = tds_model.to(device)\n","    \n","    print(\"TDS-Net model architecture:\")\n","    print(tds_model)\n","    \n","    # This would be the starting point for implementing the weakly supervised approach\n","    # with Multi-Instance Learning (MIL) loss for video anomaly detection\n","    \n","    print(\"\\nNote: To implement the full TDS-Net with weakly supervised learning,\")\n","    print(\"you would need to define the TDSNet class and train_epoch_mil function\")\n","    print(\"that uses video-level labels to infer frame-level anomalies.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7089775,"sourceId":11333890,"sourceType":"datasetVersion"},{"datasetId":7134094,"sourceId":11391601,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":1790.216289,"end_time":"2025-04-14T19:25:22.868836","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-14T18:55:32.652547","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}